# Ideias de Projetos de Portf√≥lio - Engenharia de Dados

Este reposit√≥rio cont√©m uma cole√ß√£o de ideias para projetos de portf√≥lio na √°rea de Engenharia de Dados. Cada projeto foi pensado para desenvolver e demonstrar habilidades em Big Data, arquitetura de dados, pipelines de ETL, ingest√£o de dados em tempo real, governan√ßa e machine learning.

## üéØ Objetivo

O objetivo deste reposit√≥rio √© organizar e documentar ideias de projetos que podem ser desenvolvidos futuramente para fortalecer meu portf√≥lio em Engenharia de Dados. As descri√ß√µes incluem o prop√≥sito de cada projeto, as ferramentas que poderiam ser usadas e o valor que cada um deles agregaria.

## üìÅ Estrutura das Ideias de Projetos

Cada ideia est√° documentada com uma breve descri√ß√£o, as ferramentas e tecnologias recomendadas, al√©m do objetivo espec√≠fico de aprendizado e demonstra√ß√£o de habilidades.

## ‚öôÔ∏è Projetos em Desenvolvimento

### 1. **Pipeline de ETL com Apache Airflow**
   - **Status**: Em desenvolvimento
   - **Reposit√≥rio**: [GitHub - real_state_data_pipeline](https://github.com/rafaelladuarte/real_state_data_pipeline)
   - **Descri√ß√£o**: Este projeto ter√° como objetivo desenvolver um pipeline de dados automatizado com Apache Airflow, que coletar√°, transformar√°, armazenar√° e visualizar√° dados do mercado imobili√°rio. A extra√ß√£o ser√° feita por meio de web scraping com Selenium, seguida de limpeza e c√°lculo de m√©tricas como pre√ßo m√©dio por bairro. Os dados tratados ser√£o armazenados em bancos como PostgreSQL ou MongoDB e visualizados em dashboards interativos para facilitar a an√°lise de tend√™ncias.
   - **Ferramentas**: Docker, Apache Airflow, Python (Selenium), PostgreSQL, MongoDB
   - **Objetivo**: Automatizar e escalar o processamento de dados por meio do Apache Airflow, garantindo controle de depend√™ncias, agendamento de tarefas e modularidade no pipeline de ETL.

## üí° Ideias de Projetos

### 1. **Constru√ß√£o de um Data Lake e Data Warehouse em Nuvem**
   - **Descri√ß√£o**: Criar uma arquitetura que combine um Data Lake e um Data Warehouse para armazenar dados estruturados e n√£o estruturados, otimizando o acesso aos dados.
   - **Ferramentas**: AWS (S3, Glue, Redshift), Azure Data Lake, Google BigQuery.
   - **Objetivo**: Demonstrar processos de ingest√£o de grandes volumes de dados, ETL e consultas r√°pidas para an√°lise.

### 2. **AvesRAG ‚Äì Sistema de Classifica√ß√£o Baseado em Grafo e LLM**
   - **Descri√ß√£o**: Cria√ß√£o de um sistema inteligente para classifica√ß√£o de aves com base em um grafo de conhecimento estruturado, relacionando informa√ß√µes de taxonomia, morfologia, alimenta√ß√£o, habitat e distribui√ß√£o. O grafo ser√° usado em conjunto com um modelo de linguagem (LLM), que interpretar√° descri√ß√µes fornecidas por usu√°rios e sugerir√° as esp√©cies mais prov√°veis, utilizando RAG (Retrieval-Augmented Generation).
   - **Ferramentas**: Neo4j, Python, OpenAI GPT com LangChain ou LlamaIndex, FastAPI ou Streamlit
   - **Objetivos**: Construir um grafo de conhecimento com dados ricos sobre aves, Relacionar caracter√≠sticas observ√°veis com esp√©cies reais, Permitir que um usu√°rio descreva uma ave e receba sugest√µes prov√°veis de esp√©cies, Usar RAG com LLM para classifica√ß√£o contextual baseada no grafo
   - **Base de dados**: GBIF(Taxonomia e distribui√ß√£o geogr√°fica), eBird OU Birds of the World (Nome comum, nome cient√≠fico, habitat, alimenta√ß√£o). TraitBank (Morfologia (bico, cauda, asas, etc.) e h√°bitos), Avibase (Subesp√©cies e classifica√ß√µes)

### 3. **Ingest√£o de Dados em Tempo Real com Kafka e Spark Streaming**
   - **Descri√ß√£o**: Configurar um sistema de processamento em tempo real com Apache Kafka para ingest√£o de eventos de alta frequ√™ncia e Spark Streaming.
   - **Ferramentas**: Apache Kafka, Apache Spark Streaming, HDFS, ElasticSearch.
   - **Objetivo**: Criar um pipeline de streaming para an√°lise e visualiza√ß√£o de dados em tempo real.

### 4. **Arquitetura Lambda para Processamento de Big Data**
   - **Descri√ß√£o**: Desenvolver uma arquitetura Lambda combinando processamento em lote e em tempo real.
   - **Ferramentas**: Apache Hadoop, Apache Kafka, Spark, HDFS, AWS Lambda.
   - **Objetivo**: Implementar uma arquitetura robusta para dados hist√≥ricos e eventos em tempo real.

### 5. **Processamento de Grandes Volumes de Dados com Apache Hadoop**
   - **Descri√ß√£o**: Construir um pipeline de processamento de dados distribu√≠dos em clusters HDFS.
   - **Ferramentas**: Apache Hadoop, HDFS, MapReduce, Hive.
   - **Objetivo**: Demonstrar habilidades de processamento distribu√≠do para grandes volumes de dados.

### 6. **Data Governance e Qualidade de Dados em um Data Lake**
   - **Descri√ß√£o**: Projetar uma arquitetura de Data Lake com pr√°ticas de governan√ßa, controle de qualidade, cataloga√ß√£o e versionamento.
   - **Ferramentas**: AWS Glue, Apache Atlas, AWS S3, Databricks.
   - **Objetivo**: Assegurar qualidade e governan√ßa de dados em um ambiente escal√°vel.

### 7.  **Constru√ß√£o de uma Arquitetura de Dados em Nuvem para An√°lise em Tempo Pr√≥ximo ao Real**
   - **Descri√ß√£o**: Desenvolver uma solu√ß√£o de arquitetura de dados para monitorar √¥nibus da SPTrans em S√£o Paulo, utilizando dados da API "Olho Vivo" e GTFS. O projeto incluiu a ingest√£o, processamento e visualiza√ß√£o de dados em tempo pr√≥ximo ao real, com foco em m√©tricas como geolocaliza√ß√£o, previs√£o de chegada e percentual de circula√ß√£o.
   - **Ferramentas**: Airflow (orquestra√ß√£o), Spark (processamento), MinIO (armazenamento), Metabase (visualiza√ß√£o), Presto (consultas SQL), Docker (conteineriza√ß√£o).
   - **Objetivo**: Demonstrar a constru√ß√£o de um pipeline de dados escal√°vel, desde a coleta de dados brutos at√© a cria√ß√£o de dashboards interativos, com foco em an√°lise de dados de transporte p√∫blico.
   - **Refer√™ncia**: [Festival de Ver√£o 2025 LabData FIA - Case SPTrans](https://www.youtube.com/watch?v=cLL5gppwwqA&list=PLkaqDF7JQGzLGWL6_0ZqYlEIgAqQJag5Q&index=16)

### 8. **Monitoramento de Pipelines de Dados em Big Data**
   - **Descri√ß√£o**: Desenvolver um sistema de monitoramento e alerta para pipelines de Big Data.
   - **Ferramentas**: Apache Airflow, Prometheus, Grafana.
   - **Objetivo**: Monitorar e garantir a sa√∫de de pipelines em grande escala.

### 9. **Sistema de Den√∫ncias em Tempo Real para Investiga√ß√µes Criminais**
   - **Descri√ß√£o**: Criar um sistema integrado com WhatsApp, permitindo o envio de den√∫ncias diretamente para autoridades policiais.
   - **Ferramentas**: Twilio API, Python, Elasticsearch, MongoDB, Kibana/Grafana.
   - **Objetivo**: Facilitar o envio de den√∫ncias em tempo real, centralizando o armazenamento e visualiza√ß√£o para otimizar investiga√ß√µes.
   - **Refer√™ncia**: [Hackathon: Tecnologias Disruptivas para Seguran√ßa P√∫blica](https://www.gov.br/mj/pt-br/assuntos/sua-seguranca/seguranca-publica/hackathon-tecnologias-disruptivas-para-seguranca-publica)7
      - [Apresenta√ß√µes das Equipes Vencedoras](https://www.gov.br/mj/pt-br/assuntos/sua-seguranca/seguranca-publica/confira-as-apresentacoes-das-equipes)

### 10. **OpenSky CDC DataLake**
   - **Reposit√≥rio**: [OpenSky CDC DataLake](https://github.com/rafaelladuarte/opensky-cdc-datalake)
   - **Descri√ß√£o**: Captura e replica√ß√£o de dados de tr√°fego a√©reo em tempo real, utilizando Change Data Capture (CDC) para armazenar e analisar dados de voos da OpenSky Network.
   - **Ferramentas**: PostgreSQL, Debezium, Apache Kafka, Python, mIMO (Data Lake), Docker, Trino/Iceberg.
   - **Objetivo**: Demonstrar o uso de CDC para ingest√£o de dados cont√≠nuos, garantindo armazenamento eficiente e permitindo an√°lises avan√ßadas.
   - **Refer√™ncia**: [FIA Festival CDC Data Lake](https://github.com/Labdata-FIA/fia-vestival-cdc-lake)


## üìÑ Licen√ßa

Este reposit√≥rio est√° dispon√≠vel sob a licen√ßa MIT. Sinta-se √† vontade para explorar, contribuir com ideias ou se inspirar para seus pr√≥prios projetos!
